{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ccnQmvyYmFQX"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install python-dotenv\n",
        "!pip install mysqlclient\n",
        "!pip install --pre deepchem[tensorflow]\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install paramiko\n",
        "!pip install ipython-sql\n",
        "!pip install tensorflow\n",
        "!pip install rdkit\n",
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRcRIwtknfC1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "import csv\n",
        "import io\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from time import time\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy import text\n",
        "import paramiko\n",
        "import sql\n",
        "from dotenv import dotenv_values\n",
        "import deepchem as dc\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import Draw, PyMol, rdFMCS\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit import rdBase\n",
        "from deepchem import metrics\n",
        "from IPython.display import Image, display\n",
        "from rdkit.Chem.Draw import SimilarityMaps\n",
        "import seaborn\n",
        "from matplotlib.pyplot import hist\n",
        "from sklearn.neighbors import KernelDensity\n",
        "from scipy import stats\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CRq-WZbeNS55"
      },
      "outputs": [],
      "source": [
        "# Loading the database config values for paramiko\n",
        "# paramiko connects to jean dubois VM with SSH\n",
        "\n",
        "config = dotenv_values('database_creds.env')\n",
        "\n",
        "filename = config['FILENAME']\n",
        "hostname = config['HOSTNAME']\n",
        "username = config['USERNAME']\n",
        "port = int(config['PORT'])\n",
        "password = config['PASSWORD']\n",
        "database = config['DATABASE']\n",
        "\n",
        "dubois_username = config['DUBOIS_USERNAME']\n",
        "dubois_host = config['DUBOIS_HOST']\n",
        "\n",
        "# setup client for SSH\n",
        "client = paramiko.SSHClient()\n",
        "key = paramiko.RSAKey.from_private_key_file(filename, password=password)\n",
        "client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
        "\n",
        "print(\"Connecting to SSH Client...\")\n",
        "\n",
        "try:\n",
        "    client.connect(hostname, port, username, password=password, pkey=key)\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "print(\"Successfully connected to SSH Client!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbUAPwN3keG3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# execute commands on jean dubois VM\n",
        "def exec_cmd(cmd: str, show_output=False):\n",
        "    stdin, stdout, stderr = client.exec_command(cmd)\n",
        "\n",
        "    if show_output:\n",
        "        print(stdout.read().decode())\n",
        "        print(stderr.read().decode())\n",
        "\n",
        "    return stdin, stdout, stderr\n",
        "\n",
        "\n",
        "# convert the CSV data returned by dubois back to a DataFrame (pandas)\n",
        "def csv_to_df(csv_data: str):\n",
        "    return pd.read_csv(io.StringIO(csv_data), header=0)\n",
        "\n",
        "\n",
        "# sends SELECT queries to the database provided and defaults to * if no columns are provided\n",
        "def query_db(db: str, columns: List[str] = None):\n",
        "    # formats column names to \"x, y, z\"\n",
        "    formatted_columns = re.sub(r\"[\\[\\]']\", \"\", str(columns))\n",
        "\n",
        "    # python cmd send to dubois\n",
        "    # imports sqlalchemy and pandas\n",
        "    # pandas reads the SQL and turns it into a DataFrame object\n",
        "    # converts DataFrame into csv before printing\n",
        "    multi_line_cmd = f\"\"\"\n",
        "import sqlalchemy\n",
        "from sqlalchemy import text\n",
        "import pandas as pd\n",
        "\n",
        "engine = sqlalchemy.create_engine('mysql://{dubois_username}:{password}@{dubois_host}/{database}')\n",
        "with engine.begin() as conn:\n",
        "    query = text('SELECT {\"*\" if columns is None else formatted_columns} FROM {db}')\n",
        "    data = pd.read_sql(query, conn).to_csv(index=False)\n",
        "    print(data)\n",
        "\n",
        "exit()\n",
        "    \"\"\"\n",
        "\n",
        "    # reads the output from print statement (csv data from the database)\n",
        "    std_in, stdout, stderr = exec_cmd(f'python3 -c \"{multi_line_cmd}\"', False)\n",
        "\n",
        "    return csv_to_df(stdout.read().decode())\n",
        "\n",
        "\n",
        "# queries database for matching disease_id\n",
        "def query_db_for_disease_by_id(db: str, disease_id: str):\n",
        "    query_data = query_db(db)\n",
        "    return query_data[query_data.disease_id == disease_id]\n",
        "\n",
        "\n",
        "# queries database for matching target_ensemble_id\n",
        "def query_db_for_target_by_id(db: str, target_id: str):\n",
        "    query_data = query_db(db)\n",
        "    return query_data[query_data.target_ensemble_id == target_id]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K34Nm2fGnkFA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Set disease_id variable for desired disease\n",
        "#at somepoint include api to chose what disease one wants from : https://www.ebi.ac.uk/efo/\n",
        "disease_id = \"EFO_0005537\"\n",
        "\n",
        "disease_df = query_db_for_disease_by_id(db=\"disease_to_target\", disease_id=disease_id)\n",
        "\n",
        "\n",
        "#displaying the dataframe\n",
        "display(disease_df)\n",
        "target_ids = disease_df.sort_values(by = ['association_score'], ascending=False)['target_ensemble_id'].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g24285lMTMtR"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# example target_id from target_to_compounds database\n",
        "target_id = \"ENSG00000196230\"\n",
        "\n",
        "# gets the compounds from the target\n",
        "target_to_compounds_df = query_db_for_target_by_id(db=\"target_to_compounds\", target_id=target_id)\n",
        "\n",
        "\n",
        "#show dataframe\n",
        "display(target_to_compounds_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKFpZ8uMTPRI"
      },
      "outputs": [],
      "source": [
        "%env XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/lib/cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH7HGA2_T2x4"
      },
      "outputs": [],
      "source": [
        "# 1. Train Model\n",
        "print(tf.version.VERSION)\n",
        "\n",
        "compound_dataset = target_to_compounds_df\n",
        "smiles = compound_dataset['smiles']\n",
        "IC50 = compound_dataset['standard_value']\n",
        "featurizer = dc.feat.ConvMolFeaturizer()\n",
        "compound_dataset['featurized'] = featurizer.featurize(smiles)\n",
        "compound_dataset['divided values'] = compound_dataset['standard_value'].astype(float).div(max(compound_dataset['standard_value'].astype(float)))\n",
        "compound_dataset['pIC50'] = np.log10(compound_dataset['divided values'].astype(float)).mul(-1)\n",
        "compound_dataset['number'] = list(range(0,len(compound_dataset)))\n",
        "display(compound_dataset)\n",
        "\n",
        "training_dataset = compound_dataset.sample(frac = 0.7)\n",
        "testing_dataset = (compound_dataset[~compound_dataset['number'].isin(training_dataset['number'])])\n",
        "display(testing_dataset)\n",
        "\n",
        "numpy_training_dataset = dc.data.NumpyDataset(X=training_dataset['featurized'],y=training_dataset['pIC50'].astype(float), ids=training_dataset['smiles'])\n",
        "numpy_testing_dataset = dc.data.NumpyDataset(X=testing_dataset['featurized'],y=testing_dataset['pIC50'].astype(float), ids=testing_dataset['smiles'])\n",
        "display(numpy_training_dataset)\n",
        "display(numpy_testing_dataset)\n",
        "#using  the graph convolution model\n",
        "model = dc.models.GraphConvModel(n_tasks=1, mode='regression', dropout=0.2, dense_layer_size=10)\n",
        "model.fit(numpy_training_dataset, nb_epoch=1)#was 100 on instance\n",
        "\n",
        "metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
        "print(\"Training set score:\", model.evaluate(numpy_training_dataset, [metric]))\n",
        "print(\"Test set score:\", model.evaluate(numpy_testing_dataset, [metric]))\n",
        "#end of Training the model\n",
        "\n",
        "\n",
        "#2. Download all compounds in database\n",
        "compounds_df = query_db(\"target_to_compounds\", [\"compound_id\", \"smiles\"])\n",
        "\n",
        "predict_list = []\n",
        "for smile in range(0, len(compounds_df)):\n",
        "    predict_list.append(None)\n",
        "\n",
        "compounds_df['predicted_pIC50'] = predict_list\n",
        "\n",
        "curated_compounds_df = compounds_df.dropna(subset=['smiles'])\n",
        "\n",
        "new_smiles = curated_compounds_df['smiles']\n",
        "curated_compounds_df['featurized'] = featurizer.featurize(new_smiles)\n",
        "\n",
        "#3. run the model to predict ic50 values for each compound in the database\n",
        "dataset = dc.data.NumpyDataset(X=curated_compounds_df['featurized'], y=curated_compounds_df['predicted_pIC50'], ids=curated_compounds_df['smiles'])\n",
        "curated_compounds_df['predicted_pIC50'] = model.predict(dataset)\n",
        "\n",
        "# 4. displays a dataframe that has the following two columns : compound, predicted ic50\n",
        "display(curated_compounds_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-7xh-FrpsMN"
      },
      "outputs": [],
      "source": [
        "# 5. print out the compound that has the highest ic50 value that has NOT been tested on the target already.\n",
        "\n",
        "curated_compounds_no_target = (curated_compounds_df[~curated_compounds_df['smiles'].isin(target_to_compounds_df['smiles'])])\n",
        "\n",
        "df = curated_compounds_no_target.sort_values(by = ['predicted_pIC50'], ascending=False)\n",
        "df.drop_duplicates(subset = 'smiles', inplace = True)\n",
        "\n",
        "display(df)\n",
        "compound_ids = df['compound_id'].values\n",
        "compound_smiles = df['smiles'].values\n",
        "\n",
        "compound_ids_and_smiles = list(zip(compound_ids, compound_smiles))\n",
        "best_compound = (compound_ids_and_smiles[0])\n",
        "\n",
        "# 6. get InChI key for top compound and generate a url for Zinc\n",
        "\n",
        "url = \"https://cactus.nci.nih.gov/chemical/structure/{smiles}/stdinchikey\".format(smiles = best_compound[1])\n",
        "r = requests.get(url=url)\n",
        "raw_inchikey = r.text\n",
        "inchikey = raw_inchikey.split('=')[1]\n",
        "\n",
        "print(inchikey)\n",
        "#zinc website was not working when we checked last\n",
        "zinc_url = 'https://zinc15.docking.org/substances/?inchikey={inchikey}'.format(inchikey = inchikey)\n",
        "print(zinc_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQYk1WPFqRRf"
      },
      "outputs": [],
      "source": [
        "small_df = df.head(10)\n",
        "dataset = dc.data.NumpyDataset(X=small_df['featurized'],y=small_df['predicted_pIC50'].astype(float), ids=small_df['smiles'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCcPlO_yqq8X"
      },
      "outputs": [],
      "source": [
        "just_smiles_df = pd.DataFrame()\n",
        "just_smiles_df['smiles'] = small_df['smiles']\n",
        "smiles = just_smiles_df['smiles'].tolist\n",
        "small_smiles = just_smiles_df.head(10)\n",
        "just_smiles_df['name'] = just_smiles_df['smiles']\n",
        "just_smiles_df.to_csv('smiles.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMy3R4C7qt63"
      },
      "outputs": [],
      "source": [
        "#making the smiles dataframe\n",
        "smiles = small_df['smiles']\n",
        "featurizer = dc.feat.ConvMolFeaturizer(per_atom_fragmentation = True)\n",
        "small_df['frag_featurized'] = featurizer.featurize(smiles)\n",
        "display(small_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OealPoNLq3Pm"
      },
      "outputs": [],
      "source": [
        "frag_dataset = dc.data.NumpyDataset(X=small_df['frag_featurized'], y = None, w = None, ids = dataset.ids)\n",
        "\n",
        "tr = dc.trans.FlatteningTransformer(frag_dataset) # flatten dataset and add ids to each fragment\n",
        "frag_dataset = tr.transform(frag_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EI8t0OVq6ps"
      },
      "outputs": [],
      "source": [
        "# whole molecules\n",
        "pred = model.predict(dataset)\n",
        "pred = pd.DataFrame(pred, index=dataset.ids, columns=[\"Molecule\"])  # turn to dataframe for convenience\n",
        "display(pred)\n",
        "# fragments\n",
        "pred_frags = model.predict(frag_dataset)\n",
        "pred_frags = pd.DataFrame(pred_frags, index=frag_dataset.ids, columns=[\"Fragment\"])  # turn to dataframe for convenience\n",
        "\n",
        "print(pred_frags)\n",
        "# merge 2 dataframes by molecule names\n",
        "mol_df = pd.merge(pred_frags, pred, right_index=True, left_index=True)\n",
        "# find contribs\n",
        "mol_df['Contrib'] = mol_df[\"Molecule\"] - mol_df[\"Fragment\"]\n",
        "display(mol_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvgsYK8vq_mr"
      },
      "outputs": [],
      "source": [
        "def vis_contribs(mols, df, smi_or_sdf = \"smi\"):\n",
        "    # input format of file, which was used to create dataset determines the order of atoms,\n",
        "    # so we take it into account for correct mapping!\n",
        "    maps = []\n",
        "    for mol  in mols:\n",
        "        wt = {}\n",
        "        if smi_or_sdf == \"smi\":\n",
        "            print(mol)\n",
        "            for n,atom in enumerate(Chem.rdmolfiles.CanonicalRankAtoms(mol)):\n",
        "                wt[atom] = df.loc[mol.GetProp(\"_Name\"),\"Contrib\"][n]\n",
        "\n",
        "        if smi_or_sdf == \"sdf\":\n",
        "            for n,atom in enumerate(range(mol.GetNumHeavyAtoms())):\n",
        "                wt[atom] = df.loc[Chem.MolToSmiles(mol),\"Contrib\"][n]\n",
        "        maps.append(SimilarityMaps.GetSimilarityMapFromWeights(mol,wt))\n",
        "    return maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urzoi9tJrCe7"
      },
      "outputs": [],
      "source": [
        "mols = [m for m in Chem.SmilesMolSupplier('smiles.csv', ',') if m is not None]\n",
        "print(mols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr5_bXOarDto"
      },
      "outputs": [],
      "source": [
        "vis_contribs(mols, mol_df, 'smi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRkF0WB7rWoK"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model.save_checkpoint()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-yb0WZvrkki"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "insert code here that\n",
        "1. retrieves the terms for all the assays that are relevant to the target the user picked.\n",
        "2. clusters the assays according to their descriptive terms\n",
        "3. plots the clusters (set n_clusters = 10)\n",
        "4. prints out the title of one assay from each cluster.\n",
        "\"\"\"\n",
        "#getting information from the database\n",
        "config = dotenv_values(\"database_URL.env\")\n",
        "url = config['DATABASE_URL']\n",
        "engine = create_engine(url, echo=False)\n",
        "\n",
        "with engine.begin() as conn:\n",
        "    query = text(\"select * from target_to_compounds;\")\n",
        "    target_list = pd.read_sql(query, conn)\n",
        "display(target_list)\n",
        "\n",
        "\n",
        "three_col_list = target_list[['assay_id', 'assay_description', 'abstract']].copy()\n",
        "#getting just the Assay descriptions to prep for clustering\n",
        "unique_list = []\n",
        "for index, row in three_col_list.iterrows():\n",
        "    if (row[\"assay_id\"], row[\"assay_description\"], row[\"abstract\"]) in unique_list:\n",
        "        continue\n",
        "    else:\n",
        "        unique_list.append((row[\"assay_id\"], row[\"assay_description\"], row[\"abstract\"]))\n",
        "\n",
        "Assay_Descriptions = [abstract for (assay_ids, assay_name, abstract) in unique_list]\n",
        "Assay_Descriptions_Joined = ':: '.join(Assay_Descriptions)\n",
        "#making the descriptions into one list\n",
        "Assay_Descriptions_List = Assay_Descriptions\n",
        "#the countVectorizer needed for the other vectorizer\n",
        "Assay_Count_Vect = CountVectorizer()\n",
        "Assay_Train_Counts = Assay_Count_Vect.fit_transform(Assay_Descriptions_List)\n",
        "n_clusters = 20\n",
        "X = Assay_Train_Counts.toarray()\n",
        "#the second vectorizier (tfidf transformer)\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(Assay_Train_Counts)\n",
        "tf_idf_vector = tfidf_transformer.transform(Assay_Train_Counts)\n",
        "\n",
        "ward = AgglomerativeClustering(\n",
        "    n_clusters = n_clusters, linkage=\"ward\", connectivity=None, compute_full_tree= True,compute_distances= True\n",
        ")\n",
        "ward.fit(X)\n",
        "\n",
        "unique_labels, counts = np.unique(ward.labels_, return_counts=True)\n",
        "cluster_sizes = dict(zip(unique_labels, counts))\n",
        "\n",
        "largest_cluster_label = max(cluster_sizes, key=cluster_sizes.get)\n",
        "largest_cluster_size = cluster_sizes[largest_cluster_label]\n",
        "\n",
        "print(f\"largest cluster: Cluster {largest_cluster_label}, Size: {largest_cluster_size}\")\n",
        "\n",
        "cluster_labels = ward.fit_predict(tf_idf_vector.toarray())\n",
        "\n",
        "cluster_assay_descriptions = {}\n",
        "for cluster_label, assay_description in zip(cluster_labels, Assay_Descriptions):\n",
        "    if cluster_label not in cluster_assay_descriptions:\n",
        "        cluster_assay_descriptions[cluster_label] = []\n",
        "    cluster_assay_descriptions[cluster_label].append(assay_description)\n",
        "\n",
        "sorted_clusters = sorted(cluster_assay_descriptions.keys())\n",
        "#getting and showing the cluster labels\n",
        "for cluster_label in sorted_clusters:\n",
        "    assay_descriptions = cluster_assay_descriptions[cluster_label]\n",
        "    print(f\"Cluster {cluster_label}: {assay_descriptions[0]}\")\n",
        "\n",
        "X_Hist = ward.distances_\n",
        "kernel = stats.gaussian_kde(X_Hist)\n",
        "print(kernel(X_Hist))\n",
        "\n",
        "df_idf = pd.DataFrame(tfidf_transformer.idf_,index=Assay_Count_Vect.get_feature_names_out(),columns=[\"idf_weights\"])\n",
        "df_idf.sort_values(by=['idf_weights'])\n",
        "\n",
        "count_vector=Assay_Count_Vect.transform(Assay_Descriptions_List)\n",
        "\n",
        "tf_idf_vector=tfidf_transformer.transform(count_vector)\n",
        "print(tf_idf_vector)\n",
        "\n",
        "feature_names = Assay_Count_Vect.get_feature_names_out()\n",
        "\n",
        "first_document_vector=tf_idf_vector[0]\n",
        "\n",
        "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
        "df.sort_values(by=[\"tfidf\"],ascending=False)\n",
        "#showing the Graph\n",
        "seaborn.clustermap(tf_idf_vector.toarray(),method='ward')\n",
        "\n",
        "print(len(cluster_labels))\n",
        "print(len(Assay_Descriptions))\n",
        "print(cluster_assay_descriptions[4][0] == cluster_assay_descriptions[4][1])\n",
        "print(cluster_assay_descriptions[4][9])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
